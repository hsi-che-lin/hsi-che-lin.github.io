<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hsi-Che Lin</title>
    <link rel="stylesheet" href="static/css/styles.css">
</head>
<body>

    <div class="container" style="width:65%;display:inline-block;">
        <h1>Hsi-Che Lin</h1>
        <p style="text-align:justify">
            I am currently pursuing a master's degree in the <a href="https://vllab.ee.ntu.edu.tw/">Vision Learning Lab</a>
            at National Taiwan University, advised by Prof. <a href="https://vllab.ee.ntu.edu.tw/ycwang.html">Yu-Chiang Frank Wang</a>.
        </p>
        <div class="links">
            <a href="mailto:r13942079@ntu.edu.tw" style="margin-right:3px">Email</a>|
            <a href="https://github.com/hsi-che-lin" style="margin-right:3px">GitHub</a>|
            <a href="" style="margin-right:3px">CV</a>
        </div>
    </div>
    <div class="container" style="width:25%;margin-left:5%;display:inline-block;">
        <img src="static/images/pfp.png" alt="Description of image">
    </div>

    <div class="container">
        <h2>Publications</h2>
        <div class="publication-item">
            <p class="title">EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction</p>
            <p class="authors"><u>Hsi-Che Lin</u>, Yu-Chu Yu, Kai-Po Chang, Yu-Chiang Frank Wang</p>
            <p class="venue">arXiv 2025</p>
            <div class="links">
                <a href="https://arxiv.org/abs/2506.12015">[Paper]</a>
                <a href="https://hsi-che-lin.github.io/EMLoC/">[Project Page]</a>
                <a href="https://github.com/hsi-che-lin/EMLoC">[Code]</a>
            </div>
        </div>
        <div class="publication-item">
            <p class="title">Improving Speech Emotion Recognition in Under-Resourced Languages via Speech-to-Speech Translation with Bootstrapping Data Selection</p>
            <p class="authors"><u>Hsi-Che Lin</u>*, Yi-Cheng Lin*, Huang-Cheng Chou, Hung-yi Lee</p>
            <p class="venue">ICASSP 2025</p>
            <div class="links">
                <a href="https://arxiv.org/abs/2409.10985">[Paper]</a>
                <a href="https://github.com/hsi-che-lin/Improve-SER-via-S2ST">[Code]</a>
            </div>
        </div>
        <div class="publication-item">
            <p class="title">QuAVF: Quality-aware Audio-Visual Fusion for Ego4D Talking to Me Challenge</p>
            <p class="authors"><u>Hsi-Che Lin</u>, Chien-Yi Wang, Min-Hung Chen, Szu-Wei Fu, Yu-Chiang Frank Wang</p>
            <p class="venue">1st place winner of the CVPR 2023 Ego4D Workshop Challenge</p>
            <div class="links">
                <a href="https://arxiv.org/abs/2306.17404">[Paper]</a>
                <a href="https://github.com/hsi-che-lin/Ego4D-QuAVF-TTM-CVPR23">[Code]</a>
            </div>
        </div>
        <div class="publication-item">
            <p class="title">Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks</p>
            <p class="authors">Chien-yu Huang, Wei-Chih Chen, ..., <u>Hsi-Che Lin</u>, ..., Hung-yi Lee</p>
            <p class="venue">ICLR 2025</p>
            <div class="links">
                <a href="https://arxiv.org/abs/2411.05361">[Paper]</a>
                <a href="https://dynamic-superb.github.io/">[Project Page]</a>
                <a href="https://github.com/dynamic-superb/dynamic-superb">[Code]</a>
            </div>
        </div>
        <div class="publication-item">
            <p class="title">ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos</p>
            <p class="authors">Jr-Jen Chen, Yu-Chien Liao, <u>Hsi-Che Lin</u>, Yu-Chu Yu, Yen-Chun Chen, Yu-Chiang Frank Wang</p>
            <p class="venue">NeurIPS 2024 Datasets and Benchmarks Track</p>
            <div class="links">
                <a href="https://arxiv.org/abs/2406.19392">[Paper]</a>
                <a href="https://rextime.github.io/">[Project Page]</a>
                <a href="https://github.com/ReXTime/ReXTime">[Code]</a>
            </div>
        </div>
        <div class="publication-item">
            <p class="title">On the social bias of speech self-supervised models</p>
            <p class="authors">Yi-Cheng Lin, Tzu-Quan Lin, <u>Hsi-Che Lin</u>, Andy T. Liu, Hung-yi Lee</p>
            <p class="venue">INTERSPEECH 2024</p>
            <div class="links">
                <a href="https://arxiv.org/abs/2406.04997">[Paper]</a>
            </div>
        </div>
    </div>

    <footer>
        <p>Template generated by Google Gemini.</p>
    </footer>

</body>
</html>